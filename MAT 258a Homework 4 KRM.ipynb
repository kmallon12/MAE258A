{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# MAT258a\n",
    "Kevin Mallon\n",
    "Homework 4\n",
    "Problem 4\n",
    "\n",
    "## Results\n",
    "https://docs.google.com/spreadsheets/d/1ytoKUCd6c92hpHvzg6tAaV4lvnEwJO9soxxA-Rnl68I/edit?usp=sharing\n",
    "\n",
    "## Included methods\n",
    "Currently implemented methods:\n",
    "1) newtmin: Newton's Method \n",
    "2) cgrdmin: Newton's Method with Conjugate Gradient\n",
    "3) bfgsmin: Broyden–Fletcher–Goldfarb–Shanno (Quasi-Newton)\n",
    "\n",
    "## Minimizer functions\n",
    "All minimizer functions have the following structure:\n",
    "####funcmin( obj, x0; pnum=1, maxIts=100, relTol=1e-6, absTol=1e-2)\n",
    "    Minimize a function f using a Quasi-Newton's method.\n",
    "    obj:  a function that evaluates the objective value,\n",
    "          gradient, and Hessian at a point x, for a given\n",
    "          problem, i.e., (f, g, H) = obj(x, pnum)\n",
    "    x0:   starting point\n",
    "    pnum: Problem Number. Allows one object to contain\n",
    "          multiple functions.\n",
    "    maxIts (optional): maximum number of iterations\n",
    "    relTol (optional): optimality tolerance based on\n",
    "                       ||grad(x)|| <= optTol*||grad(x0)||\n",
    "    absTol (optional): optimality tolerance based on\n",
    "                       ||grad(x)|| <= absTol\n",
    "    return(x, f, norm(g), its)\n",
    "\n",
    "## Newton's Method: Hessian Conditioning\n",
    "Hessian Conditioning:\n",
    "\n",
    "    1) Make all negative eigenvalues positive\n",
    "    2) Make all small eigenvales greater than a threshhold value, del\n",
    "    3) Vary del so that it decreases as the gradient decreases\n",
    "Note that I am modifying positive definite matrices as well as indefinite\n",
    "matrices. Positive definite matrices may still have eigenvalues very close\n",
    "to zero, so inverting that matrix can lead to very large terms. The method\n",
    "I'm using places an upper bound on B^-1, but relaxes that bound as |g|->0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "using Toms566"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "newtmin (generic function with 1 method)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function newtmin( obj, x0; pnum=1, maxIts=100, relTol=1e-6, absTol=1e-2)    \n",
    "    \n",
    "    # Initialize Process\n",
    "    its=0\n",
    "    (f, g, H) = obj(x0,pnum)\n",
    "    g0=g\n",
    "    x=x0\n",
    "\n",
    "    # Begin Minimization\n",
    "    while ((norm(g) > (norm(g0)*relTol)) || (norm(g) > absTol)) && (its <= maxIts)     \n",
    "        eps=1e-6                                # Minimum eigenvalue value\n",
    "        del=min(eps,norm(g))                    # Adjust eigenvalue minimum for small gradients\n",
    "        (L,V)=eig(H)                            # Decompose Hessian\n",
    "        Lb=max(abs(L),del)                      # Modify eigenvalues\n",
    "        Hb=V*diagm(Lb)*V'                       # Rebuild Hessian                  \n",
    "        d=-Hb\\g                                 # Determine step direction\n",
    "        alfa=armijo_search(x,d,obj,pnum)        # Backtracking linesearch\n",
    "        x=x+alfa*d                              # Take a step\n",
    "        its=its+1                               # Increase step counter\n",
    "        (f, g, H) = obj(x,pnum)                 # Evaluate function for new x\n",
    "    end\n",
    "    \n",
    "    return(x, f, norm(g), its)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "conjgrad (generic function with 1 method)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function cgrdmin( obj, x0; pnum=1, maxIts=100, relTol=1e-6, absTol=1e-2)\n",
    "  \n",
    "    # Initialize Process\n",
    "    its=0\n",
    "    (f, g, H) = obj(x0,pnum)\n",
    "    g0=g\n",
    "    x=x0\n",
    "\n",
    "    # Begin Minimization\n",
    "    while ((norm(g) > (norm(g0)*relTol)) || (norm(g) > absTol)) && (its <= maxIts)     \n",
    "        eps=1e-6                                # Minimum eigenvalue value\n",
    "        del=min(eps,norm(g))                    # Adjust eigenvalue minimum for small gradients\n",
    "        (L,V)=eig(H)                            # Decompose Hessian\n",
    "        Lb=max(abs(L),del)                      # Modify eigenvalues\n",
    "        Hb=V*diagm(Lb)*V'                       # Rebuild Hessian   \n",
    "        d=conjgrad(Hb,g)                        # Determine step direction with CG algorithm\n",
    "        alfa=armijo_search(x,d,obj,pnum)        # Backtracking linesearch\n",
    "        x=x+alfa*d                              # Take a step\n",
    "        its=its+1                               # Increase step counter\n",
    "        (f, g, H) = obj(x,pnum)                 # Evaluate function for new x\n",
    "    end\n",
    "    \n",
    "    return(x, f, norm(g), its)\n",
    "end\n",
    "\n",
    "function conjgrad(Q,b)\n",
    "    # Looking for  d that minimizes 1/2 <d,Bd>+<g,d>, \n",
    "    # analogous to x that minimizes 1/2 <x,Qx>+<b,x>.\n",
    "    # Q of the CG algorithm is H (Hessian) of the main function\n",
    "    # b of the CG algorithm is g=∇f(x) of the main funtion\n",
    "    n=length(b)     # Max iterations of CG\n",
    "    x=0.*b          # Start CG at 0\n",
    "    g=Q*x+b         # Initial gradient\n",
    "    d=-g            # Initial direction\n",
    "    k=1             # Iteration Counter\n",
    "    r=g+Q*d         # Initialize residual\n",
    "    while k<=n && (norm(r)/norm(g))>1e-3\n",
    "        a = -dot(g,d)/dot(Q*d,d)    # Compute step length\n",
    "        x =  x+a*d                  # Compute new x\n",
    "        g =  g+a*Q*d                # Compute new gradient\n",
    "        β =  dot(Q*d,g)/dot(Q*d,d)  # Compute beta\n",
    "        d = -g+β*d                  # Compute new direction\n",
    "        r =  g+Q*d                  # Compute residual\n",
    "        k =  k+1                    # Increase step counter\n",
    "    end\n",
    "    return(x) \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bfgsmin (generic function with 1 method)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function bfgsmin( obj, x0; pnum=1, maxIts=100, relTol=1e-6, absTol=1e-2)\n",
    "    \n",
    "    # Initialize Process\n",
    "    its=0\n",
    "    (f, g, H) = obj(x0,pnum)\n",
    "    g0=g\n",
    "    x=x0\n",
    "    \n",
    "    # Initialize with Gradient Descent\n",
    "    Binv=eye(length(g))                     # Start with B=I\n",
    "    d=-Binv*g                               # Determine step direction\n",
    "    (alfa,d)=wolfe_search(x,d,obj,pnum,its) # Backtracking linesearch\n",
    "    x=x+alfa*d                              # Take a step\n",
    "    its=its+1                               # Increase step counter\n",
    "    gold=g                                  # Save previous gradient\n",
    "    (f, g, H) = obj(x,pnum)                 # Evaluate function for new x\n",
    "\n",
    "    # Begin Minimization\n",
    "    while ((norm(g) > (norm(g0)*relTol)) || (norm(g) > absTol)) && (its <= maxIts)\n",
    "        s=alfa*d                            # Compute s\n",
    "        y=g-gold                            # Compute y\n",
    "        Binv=Binv+(dot(s,y)+dot(y,Binv*y))*(s*s')/dot(s,y)^2-(Binv*y*s'+s*y'*Binv)/dot(s,y) # Determine B^-1\n",
    "        d=-Binv*g\n",
    "        alfa=wolfe_search(x,d,obj,pnum)     # Backtracking linesearch\n",
    "        xold=x\n",
    "        x=x+alfa*d                          # Take a step\n",
    "        its=its+1                           # Increase step counter\n",
    "        gold=g                              # Save previous gradient\n",
    "        (f, g, H) = obj(x,pnum)             # Evaluate function for new x \n",
    "    end\n",
    "    \n",
    "    return(x, f, norm(g), its)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tomsobj (generic function with 1 method)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define objects to handle (f, g, H) = func(x)\n",
    "function tomsobj(x, pnum)\n",
    "    p=Problem(pnum)\n",
    "    f=p.obj(x)\n",
    "    g=p.grd(x)\n",
    "    H=p.hes(x)\n",
    "    n=p.n\n",
    "    return(f,g,H)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "armijo_search (generic function with 1 method)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Backtracking linesearch (Armijo)\n",
    "function armijo_search(x,d,obj,pnum)\n",
    "    mu=1e-4\n",
    "    alfa=1\n",
    "    (f0, g0) = obj(x,pnum)\n",
    "    (fn, gn) = obj(x+alfa*d,pnum)\n",
    "    while fn > f0 + alfa*mu*dot(g0,d)\n",
    "        alfa=alfa/2\n",
    "        (fn, gn) = obj(x+alfa*d,pnum)\n",
    "    end\n",
    "    return(alfa)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "wolfe_search (generic function with 2 methods)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Forward/backward linesearch (Wolfe)\n",
    "function wolfe_search(x,d,obj,pnum) \n",
    "    # First, find an alfa that meets the Armijo condition. Then,\n",
    "    # back up one step and search for an alfa between 0 and the\n",
    "    # Armijo alfa that meets the Wolfe condition. \n",
    "    \n",
    "    # Consider the Armijo condition alfa the maximum alfa and\n",
    "    # zero as the minimum alfa. Evaluate the function at the\n",
    "    # mean of the min and max alpha, and redefine the minimum\n",
    "    # or maximum alfa based on the result. Repeat this process \n",
    "    # until either the Wolfe condition has been met or 25\n",
    "    # iterations have been reached. \n",
    "    \n",
    "    # The bound on the Wolfe conditions is relaxed with each\n",
    "    # iteration. At the given maximum iterations, the min/max\n",
    "    # alpha is less than a thousandth of the original range, so\n",
    "    # at that point the goal is only to confirm if B will at \n",
    "    # least remain positive definite. \n",
    "    \n",
    "    # Armijo condition linesearch\n",
    "    mu=1e-4\n",
    "    alfa_ar=1\n",
    "    (f0, g0) = obj(x,pnum)\n",
    "    (fn, gn) = obj(x+alfa_ar*d,pnum)\n",
    "    k=1\n",
    "    while fn > f0 + alfa_ar*mu*dot(g0,d)\n",
    "        alfa_ar=0.5*alfa_ar\n",
    "        (fn,~)=obj(x+alfa_ar*d,pnum)\n",
    "    end \n",
    "    \n",
    "    # Wolfe condition linesearch\n",
    "    k=1\n",
    "    maxits=25                                           # Maximum iterations\n",
    "    eta=[logspace(-3,0,maxits);1]                       # W.C. bound: relax as k->maxits\n",
    "    alfa_min=0                                          # Initial lower bound on alfa\n",
    "    alfa_max=alfa_ar/0.5                                # Initial upper bound on alfa\n",
    "    \n",
    "    alfa=(alfa_min+alfa_max)/2                          # Initial guess for alfa\n",
    "    alfa_lo=(alfa_min+alfa)/2\n",
    "    alfa_hi=(alfa+alfa_max)/2\n",
    "    \n",
    "    (fmin,~) = obj(x+alfa_min*d,pnum)                   #                       \n",
    "    (f,g) = obj(x+alfa*d,pnum)                          # Evaluate at the guess and at each bound.                    #\n",
    "    (fmax,~) = obj(x+alfa_max*d,pnum)                   #\n",
    "    \n",
    "    while abs(dot(g,d)/dot(g0,d))>eta[k] && k<=maxits\n",
    "        if f>=fmax && f>=fmin                           # If the function value is greater at the guess than at \n",
    "            alfa=(alfa_max-alfa_min)*rand(1)+alfa_min   # either bound, make a new guess.\n",
    "        else\n",
    "            if f < fmax && f <fmin                      #\n",
    "                if fmin>fmax                            #\n",
    "                    alfa_max=alfa_hi                    #\n",
    "                else                                    #\n",
    "                    alfa_min=alfa_lo                    # \n",
    "                end                                     # Redefine bounds based on function values\n",
    "            elseif fmin < f && f < fmax                 #\n",
    "                alfa_max=alfa_hi                        #\n",
    "            elseif fmax < f && f < fmin                 #\n",
    "                alfa_min=alfa_lo                        #\n",
    "            end                                         \n",
    "            alfa=(alfa_min+alfa_max)/2                  # Make a guess between new bounds.\n",
    "        end\n",
    "        alfa=(alfa_min+alfa_max)/2                      # Initial guess for alfa\n",
    "        alfa_lo=(alfa_min+alfa)/2\n",
    "        alfa_hi=(alfa+alfa_max)/2\n",
    "    \n",
    "        (fmin,~) = obj(x+alfa_min*d,pnum)               #                       \n",
    "        (f,g) = obj(x+alfa*d,pnum)                      # Evaluate at the guess and at each bound.                       #\n",
    "        (fmax,~) = obj(x+alfa_max*d,pnum)               #\n",
    "        k=k+1\n",
    "    end\n",
    "    \n",
    "    if k==26 \n",
    "    \n",
    "    return(alfa)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed time: 2.757681846 seconds\n",
      "f*=88.03191111274238\n",
      "g*=0.009328205209947852\n",
      "g*./g0=2.847230221546132e-8\n",
      "its=260\n"
     ]
    }
   ],
   "source": [
    "# Define problem to solve, call solver\n",
    "pnum=9\n",
    "\n",
    "p = Problem(pnum)\n",
    "x0=p.x0\n",
    "g0=p.grd(x0)\n",
    "tic()\n",
    "(xs, fs, grad,k)=bfgsmin(tomsobj,x0,pnum=pnum,maxIts=300,relTol=1e-6,absTol=1e-2)\n",
    "toc()\n",
    "gg0=grad/norm(g0);\n",
    "println(\"f*=$fs\")\n",
    "println(\"g*=$grad\")\n",
    "println(\"g*./g0=$gg0\")\n",
    "println(\"its=$k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.4.0",
   "language": "julia",
   "name": "julia-0.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
